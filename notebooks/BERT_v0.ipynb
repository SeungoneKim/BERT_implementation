{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5aaccc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import wget\n",
    "\n",
    "import argparse\n",
    "import dill as pickle\n",
    "from tqdm import tqdm\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "import transformers\n",
    "from transformers import BertModel\n",
    "from transformers import BertConfig\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import datasets\n",
    "import tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8ee05fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size, num_head, sequence_length, size_per_head = key.size()\n",
    "        \n",
    "        # matmul between query and key\n",
    "        key =  key.view(batch_size, num_head, size_per_head, sequence_length)\n",
    "        \n",
    "        # scale\n",
    "        attention_score = torch.matmul(query,key) / math.sqrt(size_per_head)\n",
    "        \n",
    "        # applying mask(opt) : 0s are where we apply masking\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1) # (batch_size, 1, sequence_length, sequence_length)\n",
    "            attention_score = attention_score.masked_fill(mask==0,-1e9)\n",
    "        \n",
    "        # applying softmax\n",
    "        attention_score = F.softmax(attention_score, dim=-1)\n",
    "        \n",
    "        # matmul between attention_score and value\n",
    "        return torch.matmul(attention_score,value), attention_score\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, model_dim, key_dim, value_dim, num_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.key_dim = key_dim\n",
    "        self.value_dim = value_dim\n",
    "        self.num_head = num_head\n",
    "        \n",
    "        self.Wq = nn.Linear(model_dim, key_dim)\n",
    "        self.Wk = nn.Linear(model_dim, key_dim)\n",
    "        self.Wv = nn.Linear(model_dim, value_dim)\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "        self.Wo = nn.Linear(model_dim, model_dim)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # linearly project queries, key and values\n",
    "        prj_query = self.Wq(query)\n",
    "        prj_key = self.Wk(key)\n",
    "        prj_value = self.Wv(value)\n",
    "        \n",
    "        # split prj_query, prj_key, prj_value into multi head\n",
    "        multihead_query = self.multihead_split(prj_query)\n",
    "        multihead_key = self.multihead_split(prj_key)\n",
    "        multihead_value = self.multihead_split(prj_value)\n",
    "        \n",
    "        # perform Scaled Dot Product Attention\n",
    "        attention_output, attention_score = self.attention(multihead_query, multihead_key, multihead_value, mask=mask)\n",
    "        \n",
    "        # concat output back to 3-dimensional tensor of (batch_size, sequence_length, hidden_size)\n",
    "        output = self.multihead_concat(attention_output)\n",
    "        output = self.Wo(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def multihead_split(self, tensor):\n",
    "        batch_size, sequence_length, hidden_size = tensor.size()\n",
    "        \n",
    "        size_per_head = hidden_size // self.num_head\n",
    "        # (batch_size, num_head, sequence_length, size_per_head)\n",
    "        return tensor.view(batch_size, self.num_head, sequence_length, size_per_head)\n",
    "    \n",
    "    def multihead_concat(self, tensor):\n",
    "        batch_size, num_head, sequence_length, size_per_head = tensor.size()\n",
    "        \n",
    "        hidden_size = num_head * size_per_head\n",
    "        return tensor.view(batch_size,sequence_length,hidden_size)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, model_dim, hidden_dim, drop_prob):\n",
    "        super(FeedForward,self).__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "        self.linearlayer1 = nn.Linear(model_dim, hidden_dim)\n",
    "        self.linearlayer2 = nn.Linear(hidden_dim, model_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "    def forward(self, tensor):\n",
    "        tensor = self.dropout(self.relu(self.linearlayer1(x)))\n",
    "        return self.linearlayer2(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "898228eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, model_dim):\n",
    "        super(TokenEmbedding, self).__init__(vocab_size, model_dim, padding_idx=1)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, model_dim, max_len, device):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.encoding = torch.zeros(max_len, model_dim, device=device)\n",
    "        self.encoding.requires_grad = False\n",
    "        \n",
    "        pos = torch.arange(0,max_len,device=device).float().unsqueeze(dim=1)\n",
    "        _2i = torch.arange(0,model_dim,step=2,device=device).float()\n",
    "        \n",
    "        # self.encoding = (sequence_length, hidden_size)\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i/model_dim)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i/model_dim)))\n",
    "        \n",
    "    def forward(self, tensor):\n",
    "        batch_size, sequence_length = tensor.size()\n",
    "        \n",
    "        # (sequence_length, hidden_size)\n",
    "        return self.encoding[:sequence_length, :]\n",
    "\n",
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, model_dim, max_len, drop_prob, device):\n",
    "        super(TransformerEmbedding,self).__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, model_dim)\n",
    "        self.pos_emb = PositionalEncoding(model_dim, max_len, device)\n",
    "        self.drop_out = nn.Dropout(drop_prob)\n",
    "    \n",
    "    def forward(self, tensor):\n",
    "        tok_emb = self.tok_emb(tensor)\n",
    "        pos_emb = self.pos_emb(tensor)\n",
    "        \n",
    "        return self.drop_out(tok_emb+pos_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75d9fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, model_dim, key_dim, value_dim, hidden_dim, num_head, drop_prob):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(model_dim, key_dim, value_dim, num_head)\n",
    "        self.normalization1 = nn.LayerNorm(model_dim)\n",
    "        self.dropout1 = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.ffn = FeedForward(model_dim, hidden_dim, drop_prob)\n",
    "        self.normalization2 = nn.LayerNorm(model_dim)\n",
    "        self.dropout2 = nn.Dropout(drop_prob)\n",
    "        \n",
    "    def forward(self, tensor, source_mask):\n",
    "        residual = tensor\n",
    "        tensor = self.attention(query=tensor,key=tensor,value=tensor,mask=source_mask)\n",
    "        tensor = self.dropout1(self.normalization1(tensor+residual))\n",
    "        \n",
    "        residual = tensor\n",
    "        tensor = self.ffn(tensor)\n",
    "        tensor = self.dropout2(self.normalization2(tensor+residual))\n",
    "        \n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9aca6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, \n",
    "                    model_dim, key_dim, value_dim, hidden_dim, \n",
    "                    num_head, num_layer, drop_prob, device):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.embedding = TransformerEmbedding(vocab_size, model_dim, max_len, drop_prob, device)\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer(model_dim, key_dim, value_dim, \n",
    "                                                  hidden_dim, num_head, \n",
    "                                                  drop_prob) for _ in range(num_layer)])\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        input_emb = self.embedding(input_ids)\n",
    "        encoder_output = input_emb\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            encoder_output = layer(encoder_output, attention_mask)\n",
    "        \n",
    "        return encoder_output\n",
    "\n",
    "class NaturalLanguageUnderstandingHead(nn.Module):\n",
    "    def __init__(self, vocab_size, model_dim):\n",
    "        super(NaturalLanguageUnderstandingHead,self).__init__()\n",
    "        self.linear_layer = nn.Linear(model_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, encoder_output):\n",
    "        return F.log_softmax(self.linear_layer(encoder_output),dim=-1)\n",
    "\n",
    "class BERTModel(nn.Module):\n",
    "    def __init__(self, pad_idx, mask_idx, cls_idx, sep_idx,\n",
    "                vocab_size, model_dim, key_dim, value_dim, hidden_dim, \n",
    "                num_head, num_layer, max_len, drop_prob, device):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.mask_idx = mask_idx\n",
    "        self.cls_idx = cls_idx\n",
    "        self.sep_idx = sep_idx\n",
    "        self.device = device\n",
    "\n",
    "        self.Encoder = Encoder(vocab_size, max_len, model_dim, key_dim, value_dim, hidden_dim, num_head, num_layer, drop_prob, device)\n",
    "        self.NLUHead = NaturalLanguageUnderstandingHead(vocab_size, model_dim)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        encoder_output = self.Encoder(input_ids, token_type_ids, attention_mask)\n",
    "        output = self.NLUHead(encoder_output)\n",
    "\n",
    "        return self.NLUHead(encoder_output)\n",
    "    \n",
    "    # applying mask(opt) : 0s are where we apply masking\n",
    "    def generate_padding_mask(self, query, key, query_pad_type=None, key_pad_type=None):\n",
    "        # query = (batch_size, query_length)\n",
    "        # key = (batch_size, key_length)\n",
    "        query_length = query.size(1)\n",
    "        key_length = key.size(1)\n",
    "        \n",
    "        # convert query and key into 4-dimensional tensor\n",
    "        # query = (batch_size, 1, query_length, 1) -> (batch_size, 1, query_length, key_length)\n",
    "        # key = (batch_size, 1, 1, key_length) -> (batch_size, 1, query_length, key_length)\n",
    "        query = query.ne(query_pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "        query = query.repeat(1,1,1,key_length)\n",
    "        key = key.ne(key_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        key = key.repeat(1,1,query_length,1)\n",
    "        \n",
    "        # create padding mask with key and query\n",
    "        mask = key & query\n",
    "        \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0768855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(pad_idx, mask_idx, cls_idx, sep_idx, unk_idx,\n",
    "                vocab_size, model_dim, key_dim, value_dim, hidden_dim, \n",
    "                num_head, num_layer, max_len, drop_prob, device):\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = BERTModel(pad_idx, mask_idx, cls_idx, sep_idx,\n",
    "                vocab_size, model_dim, key_dim, value_dim, hidden_dim, \n",
    "                num_head, num_layer, max_len, drop_prob, device)\n",
    "    \n",
    "    return model.cuda() if torch.cuda.is_available() else model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61def80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters: 60304944 elements\n"
     ]
    }
   ],
   "source": [
    "tmp_model = build_model(0,103,101,102,100,30000,\n",
    "                       512,64,64,2048,8,12,1024,0.1,'cuda:0')\n",
    "\n",
    "params = list(tmp_model.parameters())\n",
    "print(\"The number of parameters:\",sum([p.numel() for p in tmp_model.parameters() if p.requires_grad]), \"elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3258794f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
